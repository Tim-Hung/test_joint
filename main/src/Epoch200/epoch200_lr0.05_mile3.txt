Script started on 2021-01-12 13:32:56+0800
tim32338519@gj939vctr1610428539698-wwn8g:~/main/src$ exitCUDA_VISIBLE_DEVICES=0 python run.py --experiment cifar --approach hat-ress50 --nepochs 200 --lr 0.05
====================================================================================================
Arguments =
	seed: 0
	experiment: cifar
	approach: hat-res50
	output: ../res/20211121332_cifar_hat-res50_0.txt
	nepochs: 200
	lr: 0.05
	parameter: 
	load_path: 
====================================================================================================
Load data...
Task order = [0]
Input size = [3, 32, 32] 
Task info = [(0, 100)]
Inits...
No pretrained model
----------------------------------------------------------------------------------------------------
Net(
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (relu): ReLU()
  (c1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c2_1_1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c2_1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c2_1_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c2_1_d): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c2_2_1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c2_2_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c2_2_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c2_3_1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c2_3_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c2_3_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c3_1_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c3_1_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (c3_1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c3_1_d): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (c3_2_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c3_2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c3_2_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c3_3_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c3_3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c3_3_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c3_4_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c3_4_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c3_4_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_1_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_1_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (c4_1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_1_d): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (c4_2_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_2_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c4_2_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_3_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c4_3_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_4_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c4_4_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_5_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c4_5_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_6_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c4_6_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c4_6_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c5_1_1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c5_1_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (c5_1_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c5_1_d): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (c5_2_1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c5_2_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c5_2_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c5_3_1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (c5_3_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c5_3_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (last): ModuleList(
    (0): Linear(in_features=2048, out_features=100, bias=True)
  )
  (gate): Sigmoid()
  (ec1): Embedding(1, 64)
  (ec2_1_1): Embedding(1, 64)
  (ec2_1_2): Embedding(1, 64)
  (ec2_1_3): Embedding(1, 256)
  (ec2_1_d): Embedding(1, 256)
  (ect2_1): Embedding(1, 256)
  (ec2_2_1): Embedding(1, 64)
  (ec2_2_2): Embedding(1, 64)
  (ec2_2_3): Embedding(1, 256)
  (ec2_3_1): Embedding(1, 64)
  (ec2_3_2): Embedding(1, 64)
  (ec2_3_3): Embedding(1, 256)
  (ec3_1_1): Embedding(1, 128)
  (ec3_1_2): Embedding(1, 128)
  (ec3_1_3): Embedding(1, 512)
  (ec3_1_d): Embedding(1, 512)
  (ect3_1): Embedding(1, 512)
  (ec3_2_1): Embedding(1, 128)
  (ec3_2_2): Embedding(1, 128)
  (ec3_2_3): Embedding(1, 512)
  (ec3_3_1): Embedding(1, 128)
  (ec3_3_2): Embedding(1, 128)
  (ec3_3_3): Embedding(1, 512)
  (ec3_4_1): Embedding(1, 128)
  (ec3_4_2): Embedding(1, 128)
  (ec3_4_3): Embedding(1, 512)
  (ec4_1_1): Embedding(1, 256)
  (ec4_1_2): Embedding(1, 256)
  (ec4_1_3): Embedding(1, 1024)
  (ec4_1_d): Embedding(1, 1024)
  (ect4_1): Embedding(1, 1024)
  (ec4_2_1): Embedding(1, 256)
  (ec4_2_2): Embedding(1, 256)
  (ec4_2_3): Embedding(1, 1024)
  (ec4_3_1): Embedding(1, 256)
  (ec4_3_2): Embedding(1, 256)
  (ec4_3_3): Embedding(1, 1024)
  (ec4_4_1): Embedding(1, 256)
  (ec4_4_2): Embedding(1, 256)
  (ec4_4_3): Embedding(1, 1024)
  (ec4_5_1): Embedding(1, 256)
  (ec4_5_2): Embedding(1, 256)
  (ec4_5_3): Embedding(1, 1024)
  (ec4_6_1): Embedding(1, 256)
  (ec4_6_2): Embedding(1, 256)
  (ec4_6_3): Embedding(1, 1024)
  (ec5_1_1): Embedding(1, 512)
  (ec5_1_2): Embedding(1, 512)
  (ec5_1_3): Embedding(1, 2048)
  (ec5_1_d): Embedding(1, 2048)
  (ect5_1): Embedding(1, 2048)
  (ec5_2_1): Embedding(1, 512)
  (ec5_2_2): Embedding(1, 512)
  (ec5_2_3): Embedding(1, 2048)
  (ec5_3_1): Embedding(1, 512)
  (ec5_3_2): Embedding(1, 512)
  (ec5_3_3): Embedding(1, 2048)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_1_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_1_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_1_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_1_d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_2_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_2_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_2_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_3_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_3_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2_3_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_1_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_1_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_1_3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_1_d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_2_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_2_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_2_3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_3_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_3_3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_4_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_4_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3_4_3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_1_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_1_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_1_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_1_d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_2_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_2_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_2_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_3_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_3_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_3_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_4_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_4_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_4_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_5_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_5_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_5_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_6_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_6_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4_6_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_1_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_1_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_1_3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_1_d): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_2_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_2_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_2_3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_3_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_3_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5_3_3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
)
Dimensions = torch.Size([64, 3, 3, 3]) torch.Size([64, 64, 1, 1]) torch.Size([64, 64, 3, 3]) torch.Size([256, 64, 1, 1]) torch.Size([256, 64, 1, 1]) torch.Size([64, 256, 1, 1]) torch.Size([64, 64, 3, 3]) torch.Size([256, 64, 1, 1]) torch.Size([64, 256, 1, 1]) torch.Size([64, 64, 3, 3]) torch.Size([256, 64, 1, 1]) torch.Size([128, 256, 1, 1]) torch.Size([128, 128, 3, 3]) torch.Size([512, 128, 1, 1]) torch.Size([512, 256, 1, 1]) torch.Size([128, 512, 1, 1]) torch.Size([128, 128, 3, 3]) torch.Size([512, 128, 1, 1]) torch.Size([128, 512, 1, 1]) torch.Size([128, 128, 3, 3]) torch.Size([512, 128, 1, 1]) torch.Size([128, 512, 1, 1]) torch.Size([128, 128, 3, 3]) torch.Size([512, 128, 1, 1]) torch.Size([256, 512, 1, 1]) torch.Size([256, 256, 3, 3]) torch.Size([1024, 256, 1, 1]) torch.Size([1024, 512, 1, 1]) torch.Size([256, 1024, 1, 1]) torch.Size([256, 256, 3, 3]) torch.Size([1024, 256, 1, 1]) torch.Size([256, 1024, 1, 1]) torch.Size([256, 256, 3, 3]) torch.Size([1024, 256, 1, 1]) torch.Size([256, 1024, 1, 1]) torch.Size([256, 256, 3, 3]) torch.Size([1024, 256, 1, 1]) torch.Size([256, 1024, 1, 1]) torch.Size([256, 256, 3, 3]) torch.Size([1024, 256, 1, 1]) torch.Size([256, 1024, 1, 1]) torch.Size([256, 256, 3, 3]) torch.Size([1024, 256, 1, 1]) torch.Size([512, 1024, 1, 1]) torch.Size([512, 512, 3, 3]) torch.Size([2048, 512, 1, 1]) torch.Size([2048, 1024, 1, 1]) torch.Size([512, 2048, 1, 1]) torch.Size([512, 512, 3, 3]) torch.Size([2048, 512, 1, 1]) torch.Size([512, 2048, 1, 1]) torch.Size([512, 512, 3, 3]) torch.Size([2048, 512, 1, 1]) torch.Size([100, 2048]) torch.Size([100]) torch.Size([1, 64]) torch.Size([1, 64]) torch.Size([1, 64]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 64]) torch.Size([1, 64]) torch.Size([1, 256]) torch.Size([1, 64]) torch.Size([1, 64]) torch.Size([1, 256]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 512]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 512]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 512]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 1024]) torch.Size([1, 1024]) torch.Size([1, 1024]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 1024]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 1024]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 1024]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 1024]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 1024]) torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 2048]) torch.Size([1, 2048]) torch.Size([1, 2048]) torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 2048]) torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 2048]) 
Num parameters = 23.7M
----------------------------------------------------------------------------------------------------
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.05
    momentum: 0
    nesterov: False
    weight_decay: 0
) = lr: 0.05, momentum: 0, dampening: 0, weight_decay: 0, nesterov: False, 
----------------------------------------------------------------------------------------------------
****************************************************************************************************
Task  0 (cifar100-all-0)
****************************************************************************************************
Train
[102, 128, 160]
2021112133259_MultiStepLR_SGD_momentum_lr0.05_factor3_task0
epochs:200
/home/tim32338519/main/src/approaches/hat.py:192: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  torch.nn.utils.clip_grad_norm(self.model.parameters(),self.clipgrad)
/home/tim32338519/main/src/approaches/hat.py:221: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  images=torch.autograd.Variable(x[b],volatile=True)
/home/tim32338519/main/src/approaches/hat.py:222: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets=torch.autograd.Variable(y[b],volatile=True)
/home/tim32338519/main/src/approaches/hat.py:223: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  task=torch.autograd.Variable(torch.LongTensor([t]).cuda(),volatile=True)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
| Epoch   1, time=203.6ms/ 76.5ms | Train: loss=4.131, acc=  9.5% | Valid: loss=4.136, acc=  9.0% |
| Epoch   2, time=201.8ms/ 76.8ms | Train: loss=3.702, acc= 15.7% | Valid: loss=3.769, acc= 14.4% |
| Epoch   3, time=201.8ms/ 76.8ms | Train: loss=3.326, acc= 23.2% | Valid: loss=3.409, acc= 21.8% |
| Epoch   4, time=202.4ms/ 76.9ms | Train: loss=3.050, acc= 28.2% | Valid: loss=3.165, acc= 25.8% |
| Epoch   5, time=202.0ms/ 76.9ms | Train: loss=2.775, acc= 33.7% | Valid: loss=2.939, acc= 30.1% |
| Epoch   6, time=202.2ms/ 76.9ms | Train: loss=2.438, acc= 40.9% | Valid: loss=2.670, acc= 35.2% |
| Epoch   7, time=202.1ms/ 76.9ms | Train: loss=2.262, acc= 44.5% | Valid: loss=2.544, acc= 37.7% |
| Epoch   8, time=202.0ms/ 76.9ms | Train: loss=2.121, acc= 47.2% | Valid: loss=2.443, acc= 39.4% |
| Epoch   9, time=202.0ms/ 76.9ms | Train: loss=1.970, acc= 51.1% | Valid: loss=2.356, acc= 42.2% |
| Epoch  10, time=202.0ms/ 76.9ms | Train: loss=1.798, acc= 54.9% | Valid: loss=2.262, acc= 43.5% |
| Epoch  11, time=202.1ms/ 76.8ms | Train: loss=1.664, acc= 57.6% | Valid: loss=2.205, acc= 46.1% |
| Epoch  12, time=202.0ms/ 76.8ms | Train: loss=1.579, acc= 59.6% | Valid: loss=2.198, acc= 46.3% |
| Epoch  13, time=202.0ms/ 76.8ms | Train: loss=1.480, acc= 62.5% | Valid: loss=2.102, acc= 48.3% |
| Epoch  14, time=202.0ms/ 76.8ms | Train: loss=1.430, acc= 63.5% | Valid: loss=2.135, acc= 46.6% |
| Epoch  15, time=202.0ms/ 76.9ms | Train: loss=1.416, acc= 63.1% | Valid: loss=2.155, acc= 46.9% |
| Epoch  16, time=202.0ms/ 76.8ms | Train: loss=1.314, acc= 66.7% | Valid: loss=2.132, acc= 48.0% |
| Epoch  17, time=202.0ms/ 76.8ms | Train: loss=1.237, acc= 68.3% | Valid: loss=2.149, acc= 47.7% |
| Epoch  18, time=202.0ms/ 77.1ms | Train: loss=1.178, acc= 70.3% | Valid: loss=2.069, acc= 51.3% |
| Epoch  19, time=205.8ms/ 78.1ms | Train: loss=1.146, acc= 71.1% | Valid: loss=2.142, acc= 49.3% |
| Epoch  20, time=202.8ms/ 77.0ms | Train: loss=1.068, acc= 72.9% | Valid: loss=2.097, acc= 50.5% |
| Epoch  21, time=202.2ms/ 76.9ms | Train: loss=1.066, acc= 73.0% | Valid: loss=2.146, acc= 49.8% |
| Epoch  22, time=202.2ms/ 76.9ms | Train: loss=0.986, acc= 75.0% | Valid: loss=2.113, acc= 49.7% |
| Epoch  23, time=202.2ms/ 77.0ms | Train: loss=1.002, acc= 74.5% | Valid: loss=2.171, acc= 50.1% |
| Epoch  24, time=202.2ms/ 77.0ms | Train: loss=0.950, acc= 76.0% | Valid: loss=2.168, acc= 50.7% |
| Epoch  25, time=202.4ms/ 77.0ms | Train: loss=0.895, acc= 77.8% | Valid: loss=2.162, acc= 50.5% |
| Epoch  26, time=202.3ms/ 77.0ms | Train: loss=0.911, acc= 77.1% | Valid: loss=2.174, acc= 50.7% |
| Epoch  27, time=202.2ms/ 76.9ms | Train: loss=0.826, acc= 79.7% | Valid: loss=2.103, acc= 51.6% |
| Epoch  28, time=202.3ms/ 76.9ms | Train: loss=0.813, acc= 80.0% | Valid: loss=2.163, acc= 51.3% |
| Epoch  29, time=202.0ms/ 76.8ms | Train: loss=0.846, acc= 79.0% | Valid: loss=2.225, acc= 50.4% |
| Epoch  30, time=202.0ms/ 76.9ms | Train: loss=0.779, acc= 81.3% | Valid: loss=2.179, acc= 50.7% |
| Epoch  31, time=202.0ms/ 76.8ms | Train: loss=0.818, acc= 80.2% | Valid: loss=2.267, acc= 49.5% |
| Epoch  32, time=202.0ms/ 76.8ms | Train: loss=0.759, acc= 81.6% | Valid: loss=2.194, acc= 50.9% |
| Epoch  33, time=202.1ms/ 76.8ms | Train: loss=0.809, acc= 79.9% | Valid: loss=2.286, acc= 49.7% |
| Epoch  34, time=202.0ms/ 76.9ms | Train: loss=0.836, acc= 79.2% | Valid: loss=2.336, acc= 49.0% |
| Epoch  35, time=202.0ms/ 76.8ms | Train: loss=0.756, acc= 81.8% | Valid: loss=2.250, acc= 50.9% |
| Epoch  36, time=202.0ms/ 76.9ms | Train: loss=0.724, acc= 82.5% | Valid: loss=2.222, acc= 50.3% |
| Epoch  37, time=202.0ms/ 76.9ms | Train: loss=0.746, acc= 81.9% | Valid: loss=2.267, acc= 51.2% |
| Epoch  38, time=202.2ms/ 76.9ms | Train: loss=0.742, acc= 81.7% | Valid: loss=2.290, acc= 49.9% |
| Epoch  39, time=202.0ms/ 76.8ms | Train: loss=0.711, acc= 82.7% | Valid: loss=2.293, acc= 50.5% |
| Epoch  40, time=202.0ms/ 76.8ms | Train: loss=0.741, acc= 82.0% | Valid: loss=2.267, acc= 50.3% |
| Epoch  41, time=202.2ms/ 76.9ms | Train: loss=0.740, acc= 81.8% | Valid: loss=2.304, acc= 49.9% |
| Epoch  42, time=202.3ms/ 76.9ms | Train: loss=0.718, acc= 82.3% | Valid: loss=2.289, acc= 50.1% |
| Epoch  43, time=202.5ms/ 77.0ms | Train: loss=0.695, acc= 83.4% | Valid: loss=2.290, acc= 50.4% |
| Epoch  44, time=202.3ms/ 76.9ms | Train: loss=0.706, acc= 82.8% | Valid: loss=2.291, acc= 50.3% |
| Epoch  45, time=202.2ms/ 76.9ms | Train: loss=0.750, acc= 81.6% | Valid: loss=2.294, acc= 50.6% |
| Epoch  46, time=202.3ms/ 76.9ms | Train: loss=0.690, acc= 83.4% | Valid: loss=2.259, acc= 50.2% |
| Epoch  47, time=202.3ms/ 76.9ms | Train: loss=0.693, acc= 83.3% | Valid: loss=2.347, acc= 50.3% |
| Epoch  48, time=202.3ms/ 76.9ms | Train: loss=0.749, acc= 81.7% | Valid: loss=2.389, acc= 48.8% |
| Epoch  49, time=202.2ms/ 76.9ms | Train: loss=0.648, acc= 85.1% | Valid: loss=2.291, acc= 51.0% |
| Epoch  50, time=202.2ms/ 76.9ms | Train: loss=0.721, acc= 82.6% | Valid: loss=2.306, acc= 50.0% |
| Epoch  51, time=202.6ms/ 76.9ms | Train: loss=0.706, acc= 83.0% | Valid: loss=2.298, acc= 50.4% |
| Epoch  52, time=202.2ms/ 76.9ms | Train: loss=0.676, acc= 83.8% | Valid: loss=2.304, acc= 51.0% |
| Epoch  53, time=202.3ms/ 76.9ms | Train: loss=0.707, acc= 83.1% | Valid: loss=2.339, acc= 50.0% |
| Epoch  54, time=202.1ms/ 76.9ms | Train: loss=0.715, acc= 82.6% | Valid: loss=2.372, acc= 49.3% |
| Epoch  55, time=202.1ms/ 76.8ms | Train: loss=0.675, acc= 83.7% | Valid: loss=2.332, acc= 50.3% |
| Epoch  56, time=202.1ms/ 76.9ms | Train: loss=0.696, acc= 83.1% | Valid: loss=2.328, acc= 49.6% |
| Epoch  57, time=202.1ms/ 76.9ms | Train: loss=0.668, acc= 84.2% | Valid: loss=2.340, acc= 49.3% |
| Epoch  58, time=202.3ms/ 76.9ms | Train: loss=0.692, acc= 83.5% | Valid: loss=2.352, acc= 49.9% |
| Epoch  59, time=202.3ms/ 76.9ms | Train: loss=0.684, acc= 83.3% | Valid: loss=2.316, acc= 50.9% |
| Epoch  60, time=202.2ms/ 76.9ms | Train: loss=0.701, acc= 83.0% | Valid: loss=2.398, acc= 50.6% |
| Epoch  61, time=201.8ms/ 76.8ms | Train: loss=0.662, acc= 84.2% | Valid: loss=2.328, acc= 50.4% |
| Epoch  62, time=201.9ms/ 76.8ms | Train: loss=0.686, acc= 83.6% | Valid: loss=2.348, acc= 49.5% |
| Epoch  63, time=203.1ms/ 76.9ms | Train: loss=0.713, acc= 82.5% | Valid: loss=2.372, acc= 49.5% |
| Epoch  64, time=202.3ms/ 76.9ms | Train: loss=0.670, acc= 83.8% | Valid: loss=2.387, acc= 49.8% |
| Epoch  65, time=202.2ms/ 76.9ms | Train: loss=0.718, acc= 82.6% | Valid: loss=2.389, acc= 49.0% |
| Epoch  66, time=202.2ms/ 76.9ms | Train: loss=0.655, acc= 84.1% | Valid: loss=2.345, acc= 49.6% |
| Epoch  67, time=202.3ms/ 76.9ms | Train: loss=0.651, acc= 84.6% | Valid: loss=2.348, acc= 50.0% |
| Epoch  68, time=201.9ms/ 76.8ms | Train: loss=0.624, acc= 85.5% | Valid: loss=2.372, acc= 49.4% |
| Epoch  69, time=201.8ms/ 76.8ms | Train: loss=0.651, acc= 84.7% | Valid: loss=2.361, acc= 50.0% |
| Epoch  70, time=201.8ms/ 76.8ms | Train: loss=0.652, acc= 84.4% | Valid: loss=2.387, acc= 49.5% |
| Epoch  71, time=201.8ms/ 76.8ms | Train: loss=0.644, acc= 85.1% | Valid: loss=2.320, acc= 50.9% |
| Epoch  72, time=201.8ms/ 76.8ms | Train: loss=0.663, acc= 84.1% | Valid: loss=2.416, acc= 48.5% |
| Epoch  73, time=201.8ms/ 76.8ms | Train: loss=0.627, acc= 85.3% | Valid: loss=2.296, acc= 51.0% |
| Epoch  74, time=201.9ms/ 76.8ms | Train: loss=0.684, acc= 83.6% | Valid: loss=2.470, acc= 48.8% |
| Epoch  75, time=201.8ms/ 76.9ms | Train: loss=0.639, acc= 84.9% | Valid: loss=2.355, acc= 51.5% |
| Epoch  76, time=201.9ms/ 76.8ms | Train: loss=0.667, acc= 84.0% | Valid: loss=2.455, acc= 49.4% |
| Epoch  77, time=201.8ms/ 76.8ms | Train: loss=0.589, acc= 86.8% | Valid: loss=2.305, acc= 51.7% |
| Epoch  78, time=201.8ms/ 76.8ms | Train: loss=0.622, acc= 85.3% | Valid: loss=2.372, acc= 50.0% |
| Epoch  79, time=201.8ms/ 76.9ms | Train: loss=0.666, acc= 83.8% | Valid: loss=2.418, acc= 50.2% |
| Epoch  80, time=202.1ms/ 76.9ms | Train: loss=0.657, acc= 84.4% | Valid: loss=2.428, acc= 50.0% |
| Epoch  81, time=202.2ms/ 76.9ms | Train: loss=0.660, acc= 84.4% | Valid: loss=2.416, acc= 49.1% |
| Epoch  82, time=202.2ms/ 76.9ms | Train: loss=0.608, acc= 85.9% | Valid: loss=2.378, acc= 50.2% |
| Epoch  83, time=202.1ms/ 76.9ms | Train: loss=0.673, acc= 83.7% | Valid: loss=2.365, acc= 50.3% |
| Epoch  84, time=202.1ms/ 76.9ms | Train: loss=0.647, acc= 84.7% | Valid: loss=2.473, acc= 48.5% |
| Epoch  85, time=202.3ms/ 77.0ms | Train: loss=0.675, acc= 83.7% | Valid: loss=2.403, acc= 50.5% |
| Epoch  86, time=202.4ms/ 76.9ms | Train: loss=0.617, acc= 85.8% | Valid: loss=2.348, acc= 50.4% |
| Epoch  87, time=202.3ms/ 76.9ms | Train: loss=0.610, acc= 85.8% | Valid: loss=2.376, acc= 50.0% |
| Epoch  88, time=202.3ms/ 76.9ms | Train: loss=0.661, acc= 84.1% | Valid: loss=2.418, acc= 49.8% |
| Epoch  89, time=202.2ms/ 76.9ms | Train: loss=0.644, acc= 84.7% | Valid: loss=2.463, acc= 49.9% |
| Epoch  90, time=202.2ms/ 76.9ms | Train: loss=0.627, acc= 85.5% | Valid: loss=2.358, acc= 50.5% |
| Epoch  91, time=202.0ms/ 76.9ms | Train: loss=0.596, acc= 86.1% | Valid: loss=2.360, acc= 50.6% |
| Epoch  92, time=202.3ms/ 76.9ms | Train: loss=0.642, acc= 84.6% | Valid: loss=2.426, acc= 48.8% |
| Epoch  93, time=202.4ms/ 77.0ms | Train: loss=0.668, acc= 83.9% | Valid: loss=2.470, acc= 48.8% |
| Epoch  94, time=202.3ms/ 77.0ms | Train: loss=0.569, acc= 87.3% | Valid: loss=2.302, acc= 50.9% |
| Epoch  95, time=202.2ms/ 76.9ms | Train: loss=0.599, acc= 86.1% | Valid: loss=2.349, acc= 51.1% |
| Epoch  96, time=202.3ms/ 77.0ms | Train: loss=0.604, acc= 86.2% | Valid: loss=2.361, acc= 51.2% |
| Epoch  97, time=202.6ms/ 77.0ms | Train: loss=0.631, acc= 85.3% | Valid: loss=2.364, acc= 51.0% |
| Epoch  98, time=202.2ms/ 76.9ms | Train: loss=0.672, acc= 84.1% | Valid: loss=2.460, acc= 49.3% |
| Epoch  99, time=202.2ms/ 77.0ms | Train: loss=0.599, acc= 86.2% | Valid: loss=2.387, acc= 50.1% |
| Epoch 100, time=202.2ms/ 77.0ms | Train: loss=0.596, acc= 86.5% | Valid: loss=2.379, acc= 49.8% |
| Epoch 101, time=202.2ms/ 76.9ms | Train: loss=0.667, acc= 84.2% | Valid: loss=2.497, acc= 49.5% |
| Epoch 102, time=202.2ms/ 76.9ms | Train: loss=0.642, acc= 85.0% | Valid: loss=2.431, acc= 50.2% |
| Epoch 103, time=202.2ms/ 77.0ms | Train: loss=0.643, acc= 84.9% | Valid: loss=2.421, acc= 49.8% |
| Epoch 104, time=202.2ms/ 77.0ms | Train: loss=0.199, acc= 99.7% | Valid: loss=1.962, acc= 57.5% |
| Epoch 105, time=202.3ms/ 76.9ms | Train: loss=0.183, acc= 99.9% | Valid: loss=1.969, acc= 58.1% |
| Epoch 106, time=202.2ms/ 77.0ms | Train: loss=0.176, acc= 99.9% | Valid: loss=1.964, acc= 58.4% |
| Epoch 107, time=202.2ms/ 76.9ms | Train: loss=0.173, acc=100.0% | Valid: loss=1.975, acc= 58.5% |
| Epoch 108, time=202.3ms/ 77.0ms | Train: loss=0.171, acc=100.0% | Valid: loss=1.969, acc= 58.5% |
| Epoch 109, time=202.2ms/ 77.0ms | Train: loss=0.169, acc=100.0% | Valid: loss=1.969, acc= 58.6% |
| Epoch 110, time=202.1ms/ 76.9ms | Train: loss=0.168, acc=100.0% | Valid: loss=1.966, acc= 58.3% |
| Epoch 111, time=202.2ms/ 77.0ms | Train: loss=0.166, acc=100.0% | Valid: loss=1.968, acc= 58.6% |
| Epoch 112, time=202.2ms/ 76.9ms | Train: loss=0.166, acc=100.0% | Valid: loss=1.965, acc= 58.6% |
| Epoch 113, time=202.1ms/ 76.9ms | Train: loss=0.165, acc=100.0% | Valid: loss=1.959, acc= 58.3% |
| Epoch 114, time=202.4ms/ 78.1ms | Train: loss=0.164, acc=100.0% | Valid: loss=1.953, acc= 58.5% |
| Epoch 115, time=205.7ms/ 78.1ms | Train: loss=0.163, acc=100.0% | Valid: loss=1.953, acc= 58.5% |
| Epoch 116, time=204.8ms/ 76.9ms | Train: loss=0.162, acc=100.0% | Valid: loss=1.955, acc= 58.6% |
| Epoch 117, time=202.3ms/ 76.9ms | Train: loss=0.162, acc=100.0% | Valid: loss=1.946, acc= 58.8% |
| Epoch 118, time=202.2ms/ 76.9ms | Train: loss=0.161, acc=100.0% | Valid: loss=1.942, acc= 58.6% |
| Epoch 119, time=202.4ms/ 76.9ms | Train: loss=0.159, acc=100.0% | Valid: loss=1.932, acc= 58.6% |
| Epoch 120, time=202.2ms/ 76.9ms | Train: loss=0.159, acc=100.0% | Valid: loss=1.935, acc= 58.5% |
| Epoch 121, time=202.2ms/ 76.9ms | Train: loss=0.158, acc=100.0% | Valid: loss=1.932, acc= 58.8% |
| Epoch 122, time=202.2ms/ 76.9ms | Train: loss=0.158, acc=100.0% | Valid: loss=1.951, acc= 58.0% |
| Epoch 123, time=202.3ms/ 76.9ms | Train: loss=0.156, acc=100.0% | Valid: loss=1.931, acc= 58.4% |
| Epoch 124, time=202.2ms/ 76.9ms | Train: loss=0.155, acc=100.0% | Valid: loss=1.938, acc= 58.0% |
| Epoch 125, time=202.2ms/ 77.0ms | Train: loss=0.155, acc=100.0% | Valid: loss=1.949, acc= 57.9% |
| Epoch 126, time=202.3ms/ 76.9ms | Train: loss=0.153, acc=100.0% | Valid: loss=1.938, acc= 57.9% |
| Epoch 127, time=202.2ms/ 76.9ms | Train: loss=0.153, acc=100.0% | Valid: loss=1.934, acc= 57.6% |
| Epoch 128, time=203.5ms/ 76.9ms | Train: loss=0.153, acc=100.0% | Valid: loss=1.946, acc= 57.2% |
| Epoch 129, time=202.2ms/ 76.9ms | Train: loss=0.152, acc=100.0% | Valid: loss=1.958, acc= 57.5% |
| Epoch 130, time=202.3ms/ 77.0ms | Train: loss=0.149, acc=100.0% | Valid: loss=1.940, acc= 57.5% |
| Epoch 131, time=202.2ms/ 77.0ms | Train: loss=0.148, acc=100.0% | Valid: loss=1.940, acc= 57.7% |
| Epoch 132, time=202.3ms/ 77.0ms | Train: loss=0.148, acc=100.0% | Valid: loss=1.944, acc= 57.5% |
| Epoch 133, time=202.2ms/ 76.9ms | Train: loss=0.148, acc=100.0% | Valid: loss=1.940, acc= 57.7% |
| Epoch 134, time=202.2ms/ 76.9ms | Train: loss=0.147, acc=100.0% | Valid: loss=1.942, acc= 57.5% |
| Epoch 135, time=202.2ms/ 76.9ms | Train: loss=0.147, acc=100.0% | Valid: loss=1.941, acc= 57.5% |
| Epoch 136, time=202.3ms/ 77.0ms | Train: loss=0.147, acc=100.0% | Valid: loss=1.942, acc= 57.6% |
| Epoch 137, time=202.2ms/ 76.9ms | Train: loss=0.147, acc=100.0% | Valid: loss=1.940, acc= 57.3% |
| Epoch 138, time=202.0ms/ 76.9ms | Train: loss=0.146, acc=100.0% | Valid: loss=1.938, acc= 57.3% |
| Epoch 139, time=202.0ms/ 76.9ms | Train: loss=0.146, acc=100.0% | Valid: loss=1.938, acc= 57.4% |
| Epoch 140, time=203.0ms/ 77.0ms | Train: loss=0.146, acc=100.0% | Valid: loss=1.940, acc= 57.3% |
| Epoch 141, time=202.3ms/ 77.0ms | Train: loss=0.146, acc=100.0% | Valid: loss=1.945, acc= 57.5% |
| Epoch 142, time=202.3ms/ 77.0ms | Train: loss=0.145, acc=100.0% | Valid: loss=1.943, acc= 57.4% |
| Epoch 143, time=202.3ms/ 77.0ms | Train: loss=0.145, acc=100.0% | Valid: loss=1.943, acc= 57.7% |
| Epoch 144, time=202.0ms/ 76.9ms | Train: loss=0.145, acc=100.0% | Valid: loss=1.944, acc= 57.5% |
| Epoch 145, time=202.0ms/ 76.9ms | Train: loss=0.145, acc=100.0% | Valid: loss=1.941, acc= 57.5% |
| Epoch 146, time=202.0ms/ 76.9ms | Train: loss=0.145, acc=100.0% | Valid: loss=1.944, acc= 57.5% |
| Epoch 147, time=202.0ms/ 76.9ms | Train: loss=0.144, acc=100.0% | Valid: loss=1.944, acc= 57.6% |
| Epoch 148, time=201.9ms/ 76.9ms | Train: loss=0.144, acc=100.0% | Valid: loss=1.953, acc= 57.5% |
| Epoch 149, time=202.0ms/ 76.9ms | Train: loss=0.144, acc=100.0% | Valid: loss=1.952, acc= 57.5% |
| Epoch 150, time=202.0ms/ 76.9ms | Train: loss=0.144, acc=100.0% | Valid: loss=1.945, acc= 57.1% |
| Epoch 151, time=202.2ms/ 77.0ms | Train: loss=0.143, acc=100.0% | Valid: loss=1.948, acc= 57.5% |
| Epoch 152, time=202.3ms/ 77.0ms | Train: loss=0.143, acc=100.0% | Valid: loss=1.955, acc= 57.5% |
| Epoch 153, time=202.0ms/ 77.0ms | Train: loss=0.143, acc=100.0% | Valid: loss=1.949, acc= 57.5% |
| Epoch 154, time=202.0ms/ 76.9ms | Train: loss=0.143, acc=100.0% | Valid: loss=1.951, acc= 57.9% |
| Epoch 155, time=201.9ms/ 76.9ms | Train: loss=0.143, acc=100.0% | Valid: loss=1.953, acc= 57.3% |
| Epoch 156, time=201.9ms/ 76.9ms | Train: loss=0.142, acc=100.0% | Valid: loss=1.948, acc= 57.5% |
| Epoch 157, time=202.0ms/ 76.9ms | Train: loss=0.142, acc=100.0% | Valid: loss=1.953, acc= 57.8% |
| Epoch 158, time=202.1ms/ 77.0ms | Train: loss=0.142, acc=100.0% | Valid: loss=1.959, acc= 57.5% |
| Epoch 159, time=202.2ms/ 76.9ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.958, acc= 57.4% |
| Epoch 160, time=202.1ms/ 76.9ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.962, acc= 57.2% |
| Epoch 161, time=202.2ms/ 77.0ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.961, acc= 57.5% |
| Epoch 162, time=202.2ms/ 77.0ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.963, acc= 57.4% |
| Epoch 163, time=202.2ms/ 77.0ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.962, acc= 57.5% |
| Epoch 164, time=202.2ms/ 77.0ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.964, acc= 57.3% |
| Epoch 165, time=202.1ms/ 77.0ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.963, acc= 57.4% |
| Epoch 166, time=202.2ms/ 77.0ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.962, acc= 57.3% |
| Epoch 167, time=202.2ms/ 77.0ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.962, acc= 57.5% |
| Epoch 168, time=202.0ms/ 77.0ms | Train: loss=0.141, acc=100.0% | Valid: loss=1.965, acc= 57.4% |
| Epoch 169, time=202.0ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.964, acc= 57.4% |
| Epoch 170, time=202.0ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.965, acc= 57.3% |
| Epoch 171, time=202.0ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.964, acc= 57.5% |
| Epoch 172, time=202.9ms/ 78.1ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.966, acc= 57.2% |
| Epoch 173, time=205.7ms/ 78.1ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.967, acc= 57.2% |
| Epoch 174, time=202.8ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.968, acc= 57.2% |
| Epoch 175, time=202.3ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.966, acc= 57.3% |
| Epoch 176, time=202.3ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.964, acc= 57.2% |
| Epoch 177, time=202.9ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.966, acc= 57.1% |
| Epoch 178, time=202.2ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.965, acc= 57.2% |
| Epoch 179, time=202.1ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.968, acc= 57.3% |
| Epoch 180, time=202.0ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.968, acc= 57.4% |
| Epoch 181, time=202.1ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.968, acc= 57.0% |
| Epoch 182, time=202.0ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.969, acc= 57.1% |
| Epoch 183, time=202.1ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.969, acc= 57.2% |
| Epoch 184, time=202.0ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.968, acc= 57.2% |
| Epoch 185, time=202.0ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.971, acc= 57.2% |
| Epoch 186, time=202.0ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.972, acc= 57.4% |
| Epoch 187, time=201.9ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.972, acc= 57.2% |
| Epoch 188, time=202.0ms/ 77.0ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.971, acc= 57.2% |
| Epoch 189, time=202.0ms/ 76.9ms | Train: loss=0.140, acc=100.0% | Valid: loss=1.973, acc= 57.0% |
| Epoch 190, time=201.9ms/ 76.9ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.970, acc= 57.3% |
| Epoch 191, time=202.0ms/ 77.0ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.972, acc= 57.0% |
| Epoch 192, time=202.0ms/ 77.0ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.972, acc= 57.3% |
| Epoch 193, time=202.0ms/ 76.9ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.973, acc= 57.3% |
| Epoch 194, time=201.9ms/ 77.0ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.971, acc= 57.2% |
| Epoch 195, time=201.9ms/ 76.9ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.974, acc= 57.0% |
| Epoch 196, time=202.1ms/ 76.9ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.974, acc= 57.0% |
| Epoch 197, time=202.0ms/ 77.0ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.974, acc= 57.0% |
| Epoch 198, time=202.1ms/ 77.0ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.974, acc= 57.1% |
| Epoch 199, time=202.0ms/ 76.9ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.973, acc= 56.9% |
| Epoch 200, time=202.0ms/ 77.0ms | Train: loss=0.139, acc=100.0% | Valid: loss=1.973, acc= 57.0% |
----------------------------------------------------------------------------------------------------
1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111
>>> Test on task  0 - cifar100-all-0 : loss=1.749, acc= 59.0% <<<
Save at ../res/20211121332_cifar_hat-res50_0.txt
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Traceback (most recent call last):
  File "run.py", line 270, in <module>
    num_test = np.size(t_list_prob,1) # 2000
  File "<__array_function__ internals>", line 6, in size
  File "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py", line 3159, in size
    return a.shape[axis]
IndexError: tuple index out of range
tim32338519@gj939vctr1610428539698-wwn8g:~/main/src$ exit
exit

Script done on 2021-01-13 11:05:36+0800
